{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebvqJaNU9bkH"
      },
      "source": [
        "# Elementy Inteligencji Obliczeniowej - Sieci Neuronowe\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Prowadzący:** Jakub Bednarek<br>\n",
        "**Kontakt:** jakub.bednarek@put.poznan.pl<br>\n",
        "**Materiały:** [Strona WWW](http://jakub.bednarek.pracownik.put.poznan.pl)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0tVMrm99g5w"
      },
      "source": [
        "## Uwaga\n",
        "\n",
        "* **Aby wykonać polecenia należy najpierw przejść do trybu 'playground'. File -> Open in Playground Mode**\n",
        "* Nowe funkcje Colab pozwalają na autouzupełnianie oraz czytanie dokumentacji"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlq47LA0BuBB"
      },
      "source": [
        "## Cel ćwiczeń:\n",
        "- zapoznanie się z rekurencyjnymi sieciami neuronowymi,\n",
        "- stworzenie modelu sieci z warstwami rekurencyjnymi dla zbioru danych MNIST,\n",
        "- stworzenie własnych implementacji warstwami neuronowych"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxLU8paIDmUe",
        "outputId": "dc0a57bd-6927-4bb4-932c-5d591074739f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scL5_bHTD-M7"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, BatchNormalization, Conv2D, MaxPooling2D, LSTM, LSTMCell, SimpleRNNCell\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adadelta, RMSprop\n",
        "from tensorflow.python.keras import backend as K\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wV_u-YBWEJ8X",
        "outputId": "432e0087-d5c4-498d-e80c-d7cabf19be8a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "\n",
        "x_train = x_train.astype('float32')  # shape: 60000, 28, 28\n",
        "x_test = x_test.astype('float32')    # shape: 10000, 28, 28\n",
        "x_train /= 255  # normalizacja wartości do przedziału [0, 1]\n",
        "x_test /= 255\n",
        "\n",
        "y_train = to_categorical(y_train, 10)  # zamiana etykiety na one-hot encoding; np. 2 -> [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
        "y_test = to_categorical(y_test, 10)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppmDSGoyFuJ9"
      },
      "source": [
        "## Sieci rekurencyjne\n",
        "http://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
        "\n",
        "https://www.tensorflow.org/guide/keras/rnn\n",
        "\n",
        "https://www.tensorflow.org/guide/function\n",
        "\n",
        "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/\n",
        "\n",
        "Przykładowy model z warstwą rekurencyjną dla danych MNIST:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViqotGlHGy9t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9ab504c-b129-48cc-c9d5-0ddf32fc3521"
      },
      "source": [
        "class RecurrentModel(Model):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModel, self).__init__(name='my_model')\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.lstm_1 = LSTM(128, activation='relu')\n",
        "        self.dense_1 = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Define your forward pass here,\n",
        "        # using layers you previously defined (in `__init__`).\n",
        "        x = self.lstm_1(inputs)\n",
        "        return self.dense_1(x)\n",
        "\n",
        "model = RecurrentModel(num_classes=10)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFs-QBFtEp0s",
        "outputId": "0a2e2e4f-41fa-49ff-a384-dc9dd5281155",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model.compile(optimizer=RMSprop(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 38s 19ms/step - loss: 0.6452 - accuracy: 0.8065\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 33s 17ms/step - loss: 0.1530 - accuracy: 0.9568\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f91543be7c0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZgtZzVYg1361"
      },
      "source": [
        "### Zadanie 1\n",
        "Rozszerz model z powyższego przykładu o kolejną warstwę rekurencyjną przed gęstą warstwą wyjściową.\n",
        "\n",
        "Standardowe sieci neuronowe generują jeden wynik na podstawie jednego inputu, natomiast sieci rekurencyjne przetwarzają dane sekwencyjnie, w każdym kroku łącząc wynik poprzedniego przetwarzania i aktualnego wejścia. Dlatego domyślnym wejściem sieci neuronowej jest tensor 3-wymiarowy ([batch_size,sequence_size,sample_size]).\n",
        "Domyślnie warstwy rekurencyjne w Kerasie zwracają tylko wyniki przetwarzania ostatniego\n",
        "kroku (otrzymują tensor 3-wymiarowy, zwracają tensor 2-wymiarowy). Jeśli chcesz zwrócić sekwencje wyników wszystkich kroków przetwarzania dla warstwy rekurencyjnej, musisz ustawić parametr return_sequences=True.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSJUzxAc15uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f06bfc75-da81-411a-ded4-c10096318b9c"
      },
      "source": [
        "class RecurrentModelZad1(Model):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModelZad1, self).__init__(name='my_model_zad1')\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.lstm1 = LSTM(128, activation='relu', return_sequences=True)\n",
        "        self.lstm2 = LSTM(128, activation='relu')\n",
        "        self.dense1 = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Define your forward pass here,\n",
        "        # using layers you previously defined (in `__init__`).\n",
        "        x = self.lstm1(inputs)\n",
        "        x = self.lstm2(x)\n",
        "        return self.dense1(x)\n",
        "\n",
        "model = RecurrentModelZad1(num_classes=10)\n",
        "model.compile(optimizer=RMSprop(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 109s 57ms/step - loss: 0.5496 - accuracy: 0.8334\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 111s 59ms/step - loss: 0.1501 - accuracy: 0.9632\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f913e7a5a00>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYDLWjdseB4H"
      },
      "source": [
        "### Zadanie 2 \n",
        "Wykorzystując model z przykładu, napisz sieć rekurencyjną przy użyciu SimpleRNNCell.\n",
        "\n",
        "Cell implementuje tylko operacje wykonywane przez warstwę\n",
        "rekurencyjną dla jednego kroku. Warstwy rekurencyjne w każdym kroku\n",
        "łączą wynik operacji poprzedniego kroku i aktualny input.\n",
        "Wykorzystaj pętle for do wielokrotnego wywołania komórki SimpleRNNCell (liczba kroków to liczba elementów w sekwencji). Aby wywołać SimpleRNNCell dla pojedynczego wejścia i stanu należy użyć jej metody ```call``` analogicznie jak w przypadku własnych modeli (tzn. ```my_model(input)```). \n",
        "\n",
        "\n",
        "\n",
        "Wywołanie zainicjalizowanej komórki rekurencyjnej wymaga podania aktualnego inputu i **listy macierzy** (w dokumentacji jest błąd, że ma to być macierz) stanów ukrytych poprzedniego kroku (SimpleRNNCell ma jeden stan, LSTMCell w następnym zadaniu ma dwa stany).\n",
        "\n",
        "Trzeba zainicjalizować ukryty stan warstwy wartościami początkowymi (można wykorzystać rozkład normalny - tf.random.normal)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yZ8yKmbee44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a28df461-d05e-455e-97d1-0791fcfe8ada"
      },
      "source": [
        "class RecurrentModelZad2(Model):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModelZad2, self).__init__(name='my_model_zad2')\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.rnn_cell1 = SimpleRNNCell(128, activation='relu')\n",
        "        self.dense1 = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Define your forward pass here,\n",
        "        # using layers you previously defined (in `__init__`).\n",
        "        h = [tf.random.normal([inputs.shape[0], 128])]\n",
        "        for i in range(inputs.shape[1]):\n",
        "          x, h = self.rnn_cell1(inputs[:,i,:], h)\n",
        "        return self.dense1(x)\n",
        "\n",
        "model = RecurrentModelZad2(num_classes=10)\n",
        "model.compile(optimizer=RMSprop(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 0.4781 - accuracy: 0.8489\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2114 - accuracy: 0.9390\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f913e1a6b80>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyPGkC6oiEd5"
      },
      "source": [
        "### Zadanie 3\n",
        "Zamień komórkę rekurencyjną z poprzedniego zadania na LSTMCell (LSTMCell ma dwa stany ukryte)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5MPQ1UcigN5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a095357a-645a-4530-a784-c5171f911459"
      },
      "source": [
        "class RecurrentModelZad3(Model):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModelZad3, self).__init__(name='my_model_zad3')\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.lstm_cell1 = LSTMCell(128, activation='relu')\n",
        "        self.dense1 = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Define your forward pass here,\n",
        "        # using layers you previously defined (in `__init__`).\n",
        "        h = [tf.random.normal([inputs.shape[0], 128]), tf.random.normal([inputs.shape[0], 128])]\n",
        "        for i in range(inputs.shape[1]):\n",
        "          x, h = self.lstm_cell1(inputs[:,i,:], h)\n",
        "        return self.dense1(x)\n",
        "\n",
        "model = RecurrentModelZad3(num_classes=10)\n",
        "model.compile(optimizer=RMSprop(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 25s 12ms/step - loss: 0.6265 - accuracy: 0.8027\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 23s 12ms/step - loss: 0.1460 - accuracy: 0.9586\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f911b68e2e0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prwjaEv2efs3"
      },
      "source": [
        "### Zadanie 4\n",
        "Wykorzystując model z poprzedniego zadania, stwórz model sieci\n",
        "neuronowej z własną implementacją prostej warstwy rekurencyjnej.\n",
        "- w call zamień self.lstm_cell_layer(x) na wywołanie własnej metody np. self.cell(x)\n",
        "- w konstruktorze modelu usuń inicjalizację komórki LSTM i zastąp ją inicjalizacją warstw potrzebnych do stworzenia własnej komórki rekurencyjnej,\n",
        "- stwórz metodę cell() wykonującą operacje warstwy rekurencyjnej,\n",
        "- prosta warstwa rekurencyjna konkatenuje poprzedni wyniki i aktualny input, a następnie przepuszcza ten połączony tensor przez warstwę gęstą (Dense)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGQr50EafxSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403fb34d-a105-4058-eef5-76b2847cd140"
      },
      "source": [
        "class RecurrentModelZad4(Model):\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(RecurrentModelZad4, self).__init__(name='my_model_zad4')\n",
        "        self.num_classes = num_classes\n",
        "        # Define your layers here.\n",
        "        self.dense1 = Dense(128, activation='relu')\n",
        "        self.dense2 = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Define your forward pass here,\n",
        "        # using layers you previously defined (in `__init__`).\n",
        "        h = tf.random.normal([inputs.shape[0], 128])\n",
        "        for i in range(inputs.shape[1]):\n",
        "          h = self.cell(inputs[:,i,:], h)\n",
        "        return self.dense2(h)\n",
        "\n",
        "    def cell(self, x, h):\n",
        "        x = K.concatenate([x, h])\n",
        "        return self.dense1(x)\n",
        "\n",
        "model = RecurrentModelZad4(num_classes=10)\n",
        "model.compile(optimizer=RMSprop(),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.5304 - accuracy: 0.8305\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.2317 - accuracy: 0.9333\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f911d110d00>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3sOaUu3b77l"
      },
      "source": [
        "### Zadanie 5\n",
        "\n",
        "Na podstawie modelu z poprzedniego zadania stwórz model z własną implementacją warstwy LSTM. Dokładny i zrozumiały opis działania warstwy LSTM znajduje się na [stronie](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kyu4YijDcA13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f78ffdb-35f1-4b34-c748-dbecdcb2ba19"
      },
      "source": [
        "class RecurrentModelZad5(Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(RecurrentModelZad5, self).__init__(name='my_model_zad5')\n",
        "        self.num_classes = num_classes\n",
        "        self.dense_forget = Dense(128, activation='sigmoid')\n",
        "        self.dense_input = Dense(128, activation='sigmoid')\n",
        "        self.dense_gate = Dense(128, activation='tanh')\n",
        "        self.dense_output = Dense(128, activation='sigmoid')\n",
        "        self.dense_5 = Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        h = [tf.random.normal([inputs.shape[0], 128]), tf.random.normal([inputs.shape[0], 128])]\n",
        "        for i in range(inputs.shape[1]):\n",
        "          x, h = self.cell(inputs[:,i,:], h)\n",
        "        return self.dense_5(x)\n",
        "\n",
        "    def cell(self, x, h):\n",
        "        c_t_1, h_t_1 = h\n",
        "        h_x = K.concatenate([h_t_1, x])\n",
        "        c_t = self.dense_forget(h_x) * c_t_1 + self.dense_input(h_x) * self.dense_gate(h_x)\n",
        "        h_t = self.dense_output(h_x) * K.tanh(c_t)\n",
        "        return h_t, (c_t, h_t)\n",
        "\n",
        "model = RecurrentModelZad5(num_classes=10)\n",
        "model.compile(optimizer=RMSprop(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=2)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "1875/1875 [==============================] - 28s 13ms/step - loss: 0.3559 - accuracy: 0.8892\n",
            "Epoch 2/2\n",
            "1875/1875 [==============================] - 24s 13ms/step - loss: 0.1184 - accuracy: 0.9644\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f911b41be20>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    }
  ]
}